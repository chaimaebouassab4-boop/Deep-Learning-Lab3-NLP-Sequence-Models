# RNN/LSTM Exam Revision Guide ğŸ”„
## Sequential Models for Arabic Text Sentiment Analysis

---

## Table of Contents
1. [Sequential Models Fundamentals](#1-sequential-models-fundamentals)
2. [RNN Architecture & Mathematics](#2-rnn-architecture--mathematics)
3. [LSTM Architecture & Mathematics](#3-lstm-architecture--mathematics)
4. [GRU Architecture & Mathematics](#4-gru-architecture--mathematics)
5. [Bidirectional RNNs](#5-bidirectional-rnns)
6. [Text Processing & Embeddings](#6-text-processing--embeddings)
7. [NLP-Specific Preprocessing](#7-nlp-specific-preprocessing)
8. [Sequence Pooling Methods](#8-sequence-pooling-methods)
9. [Advanced Training Techniques](#9-advanced-training-techniques)
10. [Evaluation Metrics for NLP](#10-evaluation-metrics-for-nlp)
11. [Key Differences Summary](#11-key-differences-summary)
12. [PyTorch Implementation Details](#12-pytorch-implementation-details)

---

## 1. Sequential Models Fundamentals

### Why Sequential Models?

Traditional feedforward neural networks (MLPs) have limitations:
- **No memory**: Each input is processed independently
- **Fixed input size**: Cannot handle variable-length sequences
- **No temporal/sequential patterns**: Cannot capture order information

**Sequential models solve this by:**
- Maintaining hidden state across time steps
- Processing variable-length sequences
- Capturing temporal dependencies and patterns

### Applications
- Natural Language Processing (text classification, translation, sentiment analysis)
- Time series prediction (stock prices, weather)
- Speech recognition
- Video analysis
- Music generation

---

## 2. RNN Architecture & Mathematics

### 2.1 Basic RNN Structure

An RNN processes sequences one element at a time, maintaining a **hidden state** that captures information from previous time steps.

**Architecture:**
```
Input: xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚œ (sequence of length T)
Hidden: hâ‚€, hâ‚, hâ‚‚, ..., hâ‚œ (hidden states)
Output: yâ‚, yâ‚‚, yâ‚ƒ, ..., yâ‚œ
```

### 2.2 RNN Mathematical Formulas

**At each time step t:**

```
hâ‚œ = tanh(Wâ‚“â‚• Â· xâ‚œ + Wâ‚•â‚• Â· hâ‚œâ‚‹â‚ + bâ‚•)
yâ‚œ = Wâ‚•áµ§ Â· hâ‚œ + báµ§
```

**Where:**
- `xâ‚œ` = input at time t (dimension: input_size)
- `hâ‚œ` = hidden state at time t (dimension: hidden_size)
- `hâ‚œâ‚‹â‚` = previous hidden state
- `hâ‚€` = initial hidden state (usually zeros)
- `Wâ‚“â‚•` = input-to-hidden weight matrix (hidden_size Ã— input_size)
- `Wâ‚•â‚•` = hidden-to-hidden weight matrix (hidden_size Ã— hidden_size)
- `Wâ‚•áµ§` = hidden-to-output weight matrix (output_size Ã— hidden_size)
- `bâ‚•, báµ§` = bias terms
- `tanh` = activation function

**Expanded Formula:**
```
hâ‚œ = tanh(Wâ‚“â‚•[xâ‚œ] + Wâ‚•â‚•[hâ‚œâ‚‹â‚] + bâ‚•)
   = tanh([xâ‚áµ—Â·wâ‚ + xâ‚‚áµ—Â·wâ‚‚ + ... + xâ‚™áµ—Â·wâ‚™] + [hâ‚áµ—â»Â¹Â·uâ‚ + hâ‚‚áµ—â»Â¹Â·uâ‚‚ + ...] + b)
```

### 2.3 RNN Unfolding Through Time

```
t=1:  xâ‚ â†’ [RNN] â†’ hâ‚ â†’ yâ‚
              â†“
t=2:  xâ‚‚ â†’ [RNN] â†’ hâ‚‚ â†’ yâ‚‚
              â†“
t=3:  xâ‚ƒ â†’ [RNN] â†’ hâ‚ƒ â†’ yâ‚ƒ
```

**Key Point**: The same weights (Wâ‚“â‚•, Wâ‚•â‚•, Wâ‚•áµ§) are shared across all time steps.

### 2.4 Backpropagation Through Time (BPTT)

To train RNNs, we use **Backpropagation Through Time**:

1. **Forward pass**: Compute all hidden states hâ‚, hâ‚‚, ..., hâ‚œ
2. **Compute loss**: L = Î£ Loss(yâ‚œ, Å·â‚œ) over all time steps
3. **Backward pass**: Propagate gradients backward through time

**Gradient computation:**
```
âˆ‚L/âˆ‚Wâ‚•â‚• = Î£â‚œ (âˆ‚L/âˆ‚hâ‚œ Â· âˆ‚hâ‚œ/âˆ‚Wâ‚•â‚•)
```

The gradient flows backward through all previous time steps.

### 2.5 Vanishing Gradient Problem âš ï¸

**Problem**: When backpropagating through many time steps:

```
âˆ‚hâ‚œ/âˆ‚hâ‚€ = âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚ Â· âˆ‚hâ‚œâ‚‹â‚/âˆ‚hâ‚œâ‚‹â‚‚ Â· ... Â· âˆ‚hâ‚/âˆ‚hâ‚€
```

If `|âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚| < 1`, the gradient vanishes:
```
(0.5)Â¹â° = 0.00097... â†’ 0
```

**Consequence**: RNN cannot learn long-term dependencies.

**Solution**: LSTM and GRU architectures (see below).

### 2.6 Exploding Gradient Problem

If `|âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚| > 1`, gradients explode:
```
(2)Â¹â° = 1024 â†’ âˆ
```

**Solution**: Gradient clipping
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## 3. LSTM Architecture & Mathematics

### 3.1 LSTM Overview

**Long Short-Term Memory (LSTM)** solves the vanishing gradient problem by introducing:
- **Memory cell** (Câ‚œ): Long-term memory
- **Three gates**: Control information flow
  1. **Forget gate** (fâ‚œ): What to forget from memory
  2. **Input gate** (iâ‚œ): What new information to add
  3. **Output gate** (oâ‚œ): What to output

### 3.2 LSTM Mathematical Formulas

**At each time step t:**

```
fâ‚œ = Ïƒ(Wâ‚“fÂ·xâ‚œ + Wâ‚•fÂ·hâ‚œâ‚‹â‚ + bf)    [Forget gate]
iâ‚œ = Ïƒ(Wâ‚“áµ¢Â·xâ‚œ + Wâ‚•áµ¢Â·hâ‚œâ‚‹â‚ + báµ¢)    [Input gate]
oâ‚œ = Ïƒ(Wâ‚“â‚’Â·xâ‚œ + Wâ‚•â‚’Â·hâ‚œâ‚‹â‚ + bâ‚’)    [Output gate]

CÌƒâ‚œ = tanh(Wâ‚“cÂ·xâ‚œ + Wâ‚•cÂ·hâ‚œâ‚‹â‚ + bc) [Candidate cell state]

Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ          [New cell state]
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)                [New hidden state]
```

**Where:**
- `Ïƒ` = sigmoid function (outputs 0-1, acts as gate)
- `âŠ™` = element-wise multiplication (Hadamard product)
- `Câ‚œ` = cell state (long-term memory)
- `hâ‚œ` = hidden state (short-term output)
- `CÌƒâ‚œ` = candidate values to add to memory

### 3.3 LSTM Gates Explained

#### Forget Gate (fâ‚œ)
```
fâ‚œ = Ïƒ(Wâ‚“fÂ·xâ‚œ + Wâ‚•fÂ·hâ‚œâ‚‹â‚ + bf)
```
- **Range**: [0, 1] due to sigmoid
- **Purpose**: Decides what to forget from Câ‚œâ‚‹â‚
- **fâ‚œ = 0**: Completely forget
- **fâ‚œ = 1**: Completely remember

#### Input Gate (iâ‚œ)
```
iâ‚œ = Ïƒ(Wâ‚“áµ¢Â·xâ‚œ + Wâ‚•áµ¢Â·hâ‚œâ‚‹â‚ + báµ¢)
CÌƒâ‚œ = tanh(Wâ‚“cÂ·xâ‚œ + Wâ‚•cÂ·hâ‚œâ‚‹â‚ + bc)
```
- **iâ‚œ**: How much of CÌƒâ‚œ to add
- **CÌƒâ‚œ**: Candidate values (range: [-1, 1])

#### Cell State Update
```
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```
- First term: What to keep from old memory
- Second term: What new information to add

#### Output Gate (oâ‚œ)
```
oâ‚œ = Ïƒ(Wâ‚“â‚’Â·xâ‚œ + Wâ‚•â‚’Â·hâ‚œâ‚‹â‚ + bâ‚’)
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
```
- **Purpose**: Decides what to output from Câ‚œ
- **tanh(Câ‚œ)**: Squash cell state to [-1, 1]
- **oâ‚œ**: Filter what parts to output

### 3.4 Why LSTM Solves Vanishing Gradients

**Key insight**: The cell state Câ‚œ has a **linear path** through time:
```
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```

The gradient can flow backward through this linear path without vanishing:
```
âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ  (element-wise multiplication, not matrix)
```

This allows gradients to flow unchanged if fâ‚œ â‰ˆ 1.

### 3.5 LSTM Visualization

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                             â”‚
xâ‚œ, hâ‚œâ‚‹â‚ â†’â”‚  fâ‚œ    iâ‚œ    CÌƒâ‚œ    oâ‚œ     â”‚â†’ hâ‚œ
          â”‚  â†“     â†“     â†“     â†“      â”‚
   Câ‚œâ‚‹â‚ â†’â”‚  Ã—  +  Ã—  =  Câ‚œ  â†’ tanh â†’ Ã—â”‚â†’ hâ‚œ
          â”‚    â†–_____â†—         â†“      â”‚
          â”‚                    â””â”€â”€â”€â”€â”€â”€â”˜â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. GRU Architecture & Mathematics

### 4.1 GRU Overview

**Gated Recurrent Unit (GRU)** is a simplified version of LSTM:
- **Fewer parameters**: Faster training, less overfitting
- **Two gates instead of three**:
  1. **Reset gate** (râ‚œ): How much past information to forget
  2. **Update gate** (zâ‚œ): How much to update hidden state
- **No separate cell state**: Only hidden state hâ‚œ

### 4.2 GRU Mathematical Formulas

```
zâ‚œ = Ïƒ(Wâ‚“zÂ·xâ‚œ + Wâ‚•zÂ·hâ‚œâ‚‹â‚ + bz)    [Update gate]
râ‚œ = Ïƒ(Wâ‚“áµ£Â·xâ‚œ + Wâ‚•áµ£Â·hâ‚œâ‚‹â‚ + báµ£)    [Reset gate]

hÌƒâ‚œ = tanh(Wâ‚“â‚•Â·xâ‚œ + Wâ‚•â‚•Â·(râ‚œ âŠ™ hâ‚œâ‚‹â‚) + bâ‚•)  [Candidate hidden state]

hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ     [New hidden state]
```

### 4.3 GRU Gates Explained

#### Update Gate (zâ‚œ)
```
zâ‚œ = Ïƒ(Wâ‚“zÂ·xâ‚œ + Wâ‚•zÂ·hâ‚œâ‚‹â‚ + bz)
```
- **Purpose**: Balance between old and new information
- **zâ‚œ = 0**: Keep old state (hâ‚œ = hâ‚œâ‚‹â‚)
- **zâ‚œ = 1**: Use new state (hâ‚œ = hÌƒâ‚œ)

#### Reset Gate (râ‚œ)
```
râ‚œ = Ïƒ(Wâ‚“áµ£Â·xâ‚œ + Wâ‚•áµ£Â·hâ‚œâ‚‹â‚ + báµ£)
hÌƒâ‚œ = tanh(Wâ‚“â‚•Â·xâ‚œ + Wâ‚•â‚•Â·(râ‚œ âŠ™ hâ‚œâ‚‹â‚) + bâ‚•)
```
- **Purpose**: Decides how much past information to use when computing hÌƒâ‚œ
- **râ‚œ = 0**: Ignore past (hÌƒâ‚œ computed only from xâ‚œ)
- **râ‚œ = 1**: Use full past information

#### Hidden State Update
```
hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ
```
- **Interpolation** between old and new state
- If zâ‚œ = 0.3: hâ‚œ = 0.7Â·hâ‚œâ‚‹â‚ + 0.3Â·hÌƒâ‚œ

### 4.4 GRU vs LSTM

| Aspect | LSTM | GRU |
|--------|------|-----|
| **Gates** | 3 (forget, input, output) | 2 (reset, update) |
| **States** | Cell (Câ‚œ) + Hidden (hâ‚œ) | Only Hidden (hâ‚œ) |
| **Parameters** | More (~4Ã—) | Fewer (~3Ã—) |
| **Speed** | Slower | Faster |
| **Performance** | Slightly better on complex tasks | Similar on most tasks |
| **Overfitting** | More prone (more params) | Less prone |

**Rule of thumb:**
- Use LSTM for: Complex sequences, large datasets
- Use GRU for: Faster training, smaller datasets

---

## 5. Bidirectional RNNs

### 5.1 Motivation

**Problem**: Standard RNNs only see past context.

**Example**: "The animal didn't cross the street because it was too ___"
- Forward RNN: Only sees words before "___"
- To predict correctly, we need future context ("tired" vs "wide")

### 5.2 Bidirectional Architecture

```
Forward:  xâ‚ â†’ hâ‚á¶  â†’ hâ‚‚á¶  â†’ hâ‚ƒá¶  â†’ hâ‚„á¶ 
                               â†“
Backward: xâ‚ â† hâ‚áµ‡ â† hâ‚‚áµ‡ â† hâ‚ƒáµ‡ â† hâ‚„áµ‡
          â†“    â†“     â†“     â†“     â†“
Output:   yâ‚   yâ‚‚    yâ‚ƒ    yâ‚„    yâ‚…
```

### 5.3 Mathematical Formulas

**Forward pass:**
```
hâ‚á¶ , hâ‚‚á¶ , ..., hâ‚œá¶  = RNN_forward(xâ‚, xâ‚‚, ..., xâ‚œ)
```

**Backward pass:**
```
hâ‚áµ‡, hâ‚‚áµ‡, ..., hâ‚œáµ‡ = RNN_backward(xâ‚œ, xâ‚œâ‚‹â‚, ..., xâ‚)
```

**Concatenate:**
```
hâ‚œ = [hâ‚œá¶  ; hâ‚œáµ‡]  (dimension: 2 Ã— hidden_size)
```

**Output:**
```
yâ‚œ = Wáµ§ Â· hâ‚œ + báµ§
```

### 5.4 Benefits & Trade-offs

**Benefits:**
- Captures both past and future context
- **Better performance** on most NLP tasks
- Essential for tasks like Named Entity Recognition, POS tagging

**Trade-offs:**
- **2Ã— parameters**: double the hidden state size
- **Cannot do online prediction**: needs entire sequence
- **Slower training**: processes sequence twice

**From your code:**
```python
self.rnn = nn.LSTM(
    embed_dim, hidden_size,
    bidirectional=True  # â† Enables Bi-LSTM
)
out_dim = hidden_size * 2  # â† Double size for concatenation
```

---

## 6. Text Processing & Embeddings

### 6.1 Word Embeddings

**Problem**: Neural networks need numerical inputs, but text is discrete.

**Solution**: Map words to dense vectors (embeddings).

**Example:**
```
"hello" â†’ [0.2, 0.5, -0.1, 0.8]
"world" â†’ [0.3, 0.4, -0.2, 0.7]
```

### 6.2 Embedding Layer Mathematics

```
E = Embedding Matrix (vocab_size Ã— embed_dim)
x = word index (integer)

embedding(x) = E[x, :]  (row lookup)
```

**Example:**
```
Vocabulary: ["<PAD>", "<UNK>", "hello", "world", "!"]
vocab_size = 5
embed_dim = 3

E = [[0.0, 0.0, 0.0],  # <PAD>
     [0.1, 0.1, 0.1],  # <UNK>
     [0.5, 0.2, 0.8],  # "hello"
     [0.6, 0.3, 0.7],  # "world"
     [0.2, 0.9, 0.4]]  # "!"

Input: [2, 3, 4] = ["hello", "world", "!"]
Output: [[0.5, 0.2, 0.8],
         [0.6, 0.3, 0.7],
         [0.2, 0.9, 0.4]]
```

**Key properties:**
- **Learnable**: E is updated during training
- **Shared**: Same embedding used throughout the model
- **Dense**: Low-dimensional representation (typically 50-300 dims)

### 6.3 Padding

**Problem**: Sequences have different lengths.

**Solution**: Pad short sequences to a fixed length.

```
Original: ["hello", "world"]
Padded:   ["hello", "world", "<PAD>", "<PAD>", "<PAD>"]

Indices:  [2, 3, 0, 0, 0]
```

**In PyTorch:**
```python
self.embedding = nn.Embedding(
    vocab_size, 
    embed_dim, 
    padding_idx=0  # â† Don't update <PAD> embedding
)
```

### 6.4 Tokenization

**Process**: Convert text to sequence of indices.

```
Text: "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"
     â†“ Tokenize
Tokens: ["Ù…Ø±Ø­Ø¨Ø§", "Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"]
     â†“ Encode
Indices: [234, 567]
```

**From your code:**
```python
class ArabicTokenizer:
    def fit(self, texts):
        # Build vocabulary from training data
        words = []
        for text in texts:
            words.extend(text.split())
        
        # Keep top vocab_size most common words
        counts = Counter(words)
        for i, (word, _) in enumerate(counts.most_common(vocab_size - 2)):
            self.word2idx[word] = i + 2  # Reserve 0, 1 for special tokens
    
    def encode(self, text, max_len=64):
        # Convert text to indices
        ids = [self.word2idx.get(w, 1) for w in text.split()[:max_len]]
        # Pad to max_len
        return ids + [0] * (max_len - len(ids))
```

---

## 7. NLP-Specific Preprocessing

### 7.1 Arabic Text Preprocessing

**Challenges in Arabic:**
1. Different forms of same letter (Ø£ØŒ Ø¥ØŒ Ø¢ â†’ Ø§)
2. Diacritics (ØªØ´ÙƒÙŠÙ„): Ù Ù Ù Ù‘ Ù’ Ù°
3. Tatweel (elongation): Ù€Ù€Ù€
4. Multiple forms of letters at word end (Ø© vs Ù‡, Ù‰ vs ÙŠ)

**Normalization steps (from your code):**
```python
def arabic_preprocess(text):
    # 1. Remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)
    
    # 2. Remove numbers (Arabic & English)
    text = re.sub(r'[0-9Ù -Ù©]+', '', text)
    
    # 3. Keep only Arabic letters and spaces
    text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
    
    # 4. Normalize Alef forms
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    
    # 5. Normalize Ya
    text = re.sub(r'Ù‰', 'ÙŠ', text)
    
    # 6. Normalize Ta Marbuta
    text = re.sub(r'Ø©', 'Ù‡', text)
    
    # 7. Remove diacritics
    text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°]', '', text)
    
    # 8. Remove tatweel
    text = re.sub(r'Ù€+', '', text)
    
    # 9. Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

### 7.2 Why Preprocessing Matters

**Example:**
```
Before: "Ø§Ù„Ø³ÙÙ‘Ù€Ù€Ù€Ù€Ù€Ù„Ø§Ù…Ù Ø¹ÙÙ„ÙŠÙ’ÙƒÙÙ…"
After:  "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…"

Before: "Ù…ÙØ­ÙÙ…ÙÙ‘Ø¯"
After:  "Ù…Ø­Ù…Ø¯"
```

**Benefits:**
- Reduces vocabulary size
- Improves generalization
- Handles spelling variations
- Removes noise (URLs, numbers)

---

## 8. Sequence Pooling Methods

After processing the sequence through RNN/LSTM, we get outputs at all time steps. For classification, we need a **single vector** representation.

### 8.1 Last Output (Default)
```
hâ‚, hâ‚‚, hâ‚ƒ, ..., hâ‚œ â†’ Use hâ‚œ only
```
- **Pros**: Simple, captures final state
- **Cons**: Ignores earlier information, sensitive to padding

### 8.2 Mean Pooling â­ (Your code uses this)
```
h_pooled = (hâ‚ + hâ‚‚ + hâ‚ƒ + ... + hâ‚œ) / T
```

**Formula:**
```
h_pooled = Î£áµ¢ háµ¢ / T
```

**With masking (ignore padding):**
```python
mask = (x != 0).unsqueeze(-1).float()  # 1 for real tokens, 0 for padding
h_pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
```

**Example:**
```
Sequence: ["hello", "world", "<PAD>", "<PAD>"]
Mask:     [1, 1, 0, 0]
Outputs:  [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„]

h_pooled = (hâ‚ + hâ‚‚) / 2  (ignores hâ‚ƒ, hâ‚„)
```

**Benefits:**
- Considers all tokens equally
- Robust to padding
- Better for sentiment analysis (all words contribute)

### 8.3 Max Pooling
```
h_pooled = max(hâ‚, hâ‚‚, ..., hâ‚œ) element-wise
```
- **Pros**: Captures strongest features
- **Cons**: Can ignore important information

### 8.4 Attention Pooling
```
Î±â‚, Î±â‚‚, ..., Î±â‚œ = Attention(hâ‚, hâ‚‚, ..., hâ‚œ)
h_pooled = Î±â‚Â·hâ‚ + Î±â‚‚Â·hâ‚‚ + ... + Î±â‚œÂ·hâ‚œ
```
- **Pros**: Learns which tokens are important
- **Cons**: More complex, more parameters

---

## 9. Advanced Training Techniques

### 9.1 Gradient Clipping

**Problem**: Exploding gradients in RNNs.

**Solution**: Clip gradient norm to maximum value.

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Mathematical formula:**
```
If ||g|| > max_norm:
    g_clipped = g Ã— (max_norm / ||g||)
else:
    g_clipped = g
```

Where `||g||` is the L2 norm of all gradients.

**Why it works:**
- Limits gradient magnitude
- Prevents explosive updates
- Allows training to converge

### 9.2 Learning Rate Scheduling

**ReduceLROnPlateau** (from your code):
```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    patience=3,    # Wait 3 epochs
    factor=0.5     # Multiply LR by 0.5
)
```

**How it works:**
```
Epoch 1: LR = 0.001, Val Loss = 0.5
Epoch 2: LR = 0.001, Val Loss = 0.48
Epoch 3: LR = 0.001, Val Loss = 0.47
Epoch 4: LR = 0.001, Val Loss = 0.46  â† Still improving
Epoch 5: LR = 0.001, Val Loss = 0.46  â† No improvement (1/3)
Epoch 6: LR = 0.001, Val Loss = 0.46  â† No improvement (2/3)
Epoch 7: LR = 0.001, Val Loss = 0.46  â† No improvement (3/3)
Epoch 8: LR = 0.0005 â† Reduced! (0.001 Ã— 0.5)
```

**Benefits:**
- Automatic adaptation
- Fine-tunes in later stages
- Helps escape plateaus

### 9.3 Early Stopping

**Algorithm:**
```python
class EarlyStopping:
    def __init__(self, patience=7):
        self.patience = patience
        self.counter = 0
        self.best_loss = None
        self.best_model = None
    
    def __call__(self, val_loss, model):
        if self.best_loss is None or val_loss < self.best_loss - 0.001:
            # Improvement found
            self.best_loss = val_loss
            self.best_model = copy of model
            self.counter = 0
        else:
            # No improvement
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Stop training
        return False
```

**Example:**
```
Epoch 1: Val Loss = 0.500 â†’ Save (best so far)
Epoch 2: Val Loss = 0.450 â†’ Save (improved!)
Epoch 3: Val Loss = 0.445 â†’ Save
Epoch 4: Val Loss = 0.446 â†’ No save (counter = 1)
Epoch 5: Val Loss = 0.447 â†’ No save (counter = 2)
...
Epoch 10: Val Loss = 0.450 â†’ No save (counter = 7) â†’ STOP!
```

### 9.4 Layer Normalization

**Formula:**
```
y = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

Where:
- Î¼ = mean of x
- ÏƒÂ² = variance of x
- Î³, Î² = learnable parameters
- Îµ = small constant (1e-5) for numerical stability

**Applied to embeddings (from your code):**
```python
emb = self.embedding(x)          # (batch, seq_len, embed_dim)
emb = self.layer_norm(emb)       # Normalize across embed_dim
```

**Benefits:**
- Stabilizes training
- Allows higher learning rates
- Reduces internal covariate shift

### 9.5 Weight Decay (L2 Regularization)

**AdamW optimizer** (from your code):
```python
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=1e-3, 
    weight_decay=1e-4  # â† L2 regularization
)
```

**Effect:**
```
Loss = MSE_loss + Î» Ã— Î£(wÂ²)
      = MSE_loss + 1e-4 Ã— Î£(wÂ²)
```

**Gradient update:**
```
w = w - lr Ã— (âˆ‚MSE/âˆ‚w + 2Î»w)
  = w - 1e-3 Ã— (âˆ‚MSE/âˆ‚w + 2Ã—1e-4Ã—w)
```

---

## 10. Evaluation Metrics for NLP

### 10.1 Regression Metrics (Sentiment Scores)

Your task predicts continuous scores (e.g., 1-10 rating).

#### Mean Absolute Error (MAE)
```
MAE = (1/n) Î£ |y_true - y_pred|
```
- **Interpretation**: Average absolute difference
- **Lower is better**
- **Example**: MAE = 0.5 means predictions are off by 0.5 points on average

#### Root Mean Squared Error (RMSE)
```
RMSE = âˆš[(1/n) Î£ (y_true - y_pred)Â²]
```
- Penalizes large errors more than MAE
- Same units as target variable

#### RÂ² Score
```
RÂ² = 1 - (SS_residual / SS_total)
   = 1 -